---
title: Bibliography
layout: default
navigation_weight: 2
---
## :books: Bibliography :books:

This course will not follow a single specific source. Each lecture will have links to corresponding material and I will also list some relevant works here.

A comprehensive overview is available at:

* RESNIK, Philip & LIN, Jimmy. Evaluation of NLP Systems. In: Clark, Alexander; Fox, Chris & Lappin, Shalom (Eds.). The handbook of computational linguistics and natural language processing. John Wiley & Sons, pp. 271-295, 2010.

Books & book chapters

* DROR, Rotem; PELED-COHEN, Lotem; SHLOMOV, Segev & REICHART, Roi. Statistical Significance Testing for Natural Language Processing. Synthesis Lectures on Human Language Technologies, v. 13, n. 2, p. 1-116, 2020. [link](https://www.morganclaypool.com/doi/abs/10.2200/S00994ED1V01Y202002HLT045?casa_token=sFwy5BwuTSYAAAAA:wfzPR418bHmo7Pt_T1LEzL2SSVI648i2MIFGFEwNK1NKglsjq8cOYMRfHmPCk8Qo0EsW4pi9vvij)
* GALLIERS, Julia R.; SPÄRCK JONES, Karen. Evaluating natural language processing systems. University of Cambridge, Computer Laboratory, 1993.
* HIRSCHMAN, Lynette; THOMPSON, Henry S. Overview of evaluation in speech and natural language processing. In: Survey of the state of the art in human language technology. Cambridge University Press, 1997. p. 409-414.
* (*Experimentation*, Appendix B of) SMITH, Noah A. Linguistic Structure Prediction. Synthesis Lectures on Human Language Technologies, v. 4, n. 2, 2011. 
[link](https://doi.org/10.2200/S00361ED1V01Y201105HLT013) 
* SPÄRCK JONES, Karen & GALLIERS, Julia R. Evaluating Natural Language Processing Systems: An Analysis and Review. Berlin: Springer, 1996. [link](https://opac.ub.uni-potsdam.de/DB=1/SET=3/TTL=2/SHW?FRST=1)


Papers

* BARR, Valerie; KLAVANS, Judith L. Verification and validation of language processing systems: is it evaluation?. In: Proceedings of the ACL 2001 Workshop on Evaluation Methodologies for Language and Dialogue Systems. 2001. [link](https://www.aclweb.org/anthology/W01-0906.pdf)
* BELINKOV, Yonatan & GLASS, James. Analysis methods in neural language processing: A survey. Transactions of the Association for Computational Linguistics, v. 7, p. 49-72, 2019. [link](https://www.aclweb.org/anthology/Q19-1004.pdf)
* BELZ, Anja. That's nice… what can you do with it?. Computational Linguistics, v. 35, n. 1, p. 111-118, 2009.
* BENDER, Emily M.; FRIEDMAN, Batya. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, v. 6, p. 587-604, 2018. [link](https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00041)
* BERG-KIRKPATRICK, Taylor; BURKETT, David; KLEIN, Dan. An empirical investigation of statistical significance in nlp. In: Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, p. 995-1005, 2012. [link](https://www.aclweb.org/anthology/D12-1091.pdf)
* DROR, Rotem; BAUMER, Gili; BOGOMOLOV, Marina & REICHART, Roi. Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets. Transactions of the Association for Computational Linguistics, v. 5, p. 471-486, 2017. [link](https://www.aclweb.org/anthology/Q17-1033.pdf)
* DROR, R., BAUMER, G., SHLOMOV, S., & REICHART, R. The hitchhiker’s guide to testing statistical significance in natural language processing. In: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), p. 1383-1392, 2018. [link](https://www.aclweb.org/anthology/P18-1128/)
* ESCARTÍN, Carla Parra et al. Ethical Considerations in NLP Shared Tasks. In: Proceedings of the First ACL Workshop on Ethics in Natural Language Processing. 2017. p. 66-73. [link](https://www.aclweb.org/anthology/W17-1608.pdf)
* FORT, Karën; ADDA, Gilles; COHEN, K. Bretonnel. Amazon mechanical turk: Gold mine or coal mine?. Computational Linguistics, v. 37, n. 2, p. 413-420, 2011. [link](https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00057)
* GORMAN, Kyle; BEDRICK, Steven. We need to talk about standard splits. In: Proceedings of the 57th annual meeting of the association for computational linguistics. 2019. p. 2786-2791. [link](https://www.aclweb.org/anthology/P19-1267.pdf)
* HOVY, Dirk; SPRUIT, Shannon L. The social impact of natural language processing. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2016. p. 591-598. [link](https://www.aclweb.org/anthology/P16-2096.pdf)
* KING, Margaret. Evaluating natural language processing systems. Communications of the ACM, v. 39, n. 1, p. 73-79, 1996. [link](https://dl.acm.org/doi/abs/10.1145/234173.234208?casa_token=CwCV7waCKFIAAAAA:xhvxEl6RDm57vO0Oq9kqhCFWbAGz5yrdi0d9RlLnb-sRM8l4fEMaJhPcuXfkd0ps5yREAxvQDHrt)
* NOVIKOVA, Jekaterina et al. Why We Need New Evaluation Metrics for NLG. In: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, p. 2241-2252, 2017. [link](https://www.aclweb.org/anthology/D17-1238/)
* PAROUBEK, Patrick; CHAUDIRON, Stéphane & HIRSCHMAN, Lynette. Principles of Evaluation in Natural Language Processing. Traitement Automatique des Langues, ATALA, 48 (1), pp.7-31. hal- 00502700, 2007.
* SØGAARD, Anders; JOHANNSEN, Anders; PLANK, Barbara; HOVY, Dirk & MARTINEZ, Hector. What’s in a p-value in NLP?. In: Proceedings of the eighteenth conference on computational natural language learning, p. 1-10, 2014. [link](https://www.aclweb.org/anthology/W14-1601.pdf)
* SPÄRCK JONES, Karen. Towards better NLP system evaluation. In: Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994. [link](https://www.aclweb.org/anthology/H94-1018.pdf)
* VAN DER LEE, C., GATT, A., VAN MILTENBURG, E., WUBBEN, S., & KRAHMER, E.  Best practices for the human evaluation of automatically generated text. In: Proceedings of the 12th International Conference on Natural Language Generation, p. 355-368, 2019. [link](https://www.aclweb.org/anthology/W19-8643.pdf)


Miscellaneous

* COHEN, Paul R.; HOWE, Adele E. How evaluation guides AI research: The message still counts more than the medium. AI magazine, v. 9, n. 4, p. 35-35, 1988. [link](https://doi.org/10.1609/aimag.v9i4.952)
* HUYEN, Chip. Evaluation Metrics for Language Modeling. Blogpost, 2019. [link](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/)
* KING, Margaret. Evaluating natural language processing systems. Communications of the ACM, v. 39, n. 1, p. 73-79, 1996. [link](https://dl.acm.org/doi/abs/10.1145/234173.234208?casa_token=8KZTFYtxcXoAAAAA:x71qSj6riN8FnypzsDWZ3n8qIV8b0C5H14ToxWdQlLAYMqbWytVdAxmELv0QhyvuYfgCFFHhcYK_)
* KING M., Maegaard B., Schütz J., des Tombes L., Bech A., Neville A., Arppe A., Balkan L., Brace C., Bunt H., Carlson L., Douglas S., Höge M., Krauwer S., Manzi S., Mazzi, C., Sieleman A. J., Steenbakkers R. EAGLES Evaluation of Natural Language Processing Systems: Final Report. EAGLES Document EAGEWG-PR. 2. Center for Sprogteknologi, Copenhagen, 1996. [link](https://www.issco.unige.ch/en/research/projects/ewg96/node1.html)
* LIPTON, Zachary C.; STEINHARDT, Jacob. Troubling trends in machine learning scholarship. Queue, v. 17, n. 1, p. 45-77, 2019. [link](https://queue.acm.org/detail.cfm?id=3328534)
* POTTS, Christopher. Evaluation methods and metrics in NLP. 2020. [link](https://nbviewer.jupyter.org/github/cgpotts/cs224u/blob/master/evaluation_methods.ipynb) and [link](https://nbviewer.jupyter.org/github/cgpotts/cs224u/blob/master/evaluation_metrics.ipynb)
* REITER, Ehud. His blog has several posts related to evaluation. [link](https://ehudreiter.com/)


### Group presentations

Dialogue

* DERIU, Jan et al. Survey on evaluation methods for dialogue systems. Artificial Intelligence Review, p. 1-56, 2020. [link](https://link.springer.com/article/10.1007/s10462-020-09866-x)
* LIU, Chia-Wei et al. How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation. In: Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. 2016. p. 2122-2132. [link](https://www.aclweb.org/anthology/D16-1230.pdf)
* DUSEK, O \& HUDECEK, V. Course about Dialogue Systems, 2019. [link](http://ufal.mff.cuni.cz/courses/npfl123)

Machine Translation

* CALLISON-BURCH, Chris et al. (Meta-) evaluation of machine translation. In: Proceedings of the Second Workshop on Statistical Machine Translation. 2007. p. 136-158. [link](https://www.aclweb.org/anthology/W07-0718.pdf)
* CHATZIKOUMI, Eirini. How to evaluate machine translation: A review of automated and human metrics. Natural Language Engineering, v. 26, n. 2, p. 137-161, 2020. [link](https://www.cambridge.org/core/journals/natural-language-engineering/article/how-to-evaluate-machine-translation-a-review-of-automated-and-human-metrics/1005E82E4A0D7AD6F0FACA2F00083997)
* MÜLLER, Mathias. Seven recommendations for machine translation evaluation. Blogpost on Dec 15, 2020. [link](https://bricksdont.github.io/posts/2020/12/seven-recommendations-for-mt-evaluation/?utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter)
* RIEZLER, Stefan; MAXWELL III, John T. On some pitfalls in automatic evaluation and significance testing for MT. In: Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. 2005. p. 57-64. [link](https://www.aclweb.org/anthology/W05-0908.pdf)

Natural Language Generation

* CELIKYILMAZ, Asli; CLARK, Elizabeth; GAO, Jianfeng. Evaluation of Text Generation: A Survey. Preprint.  [link](https://arxiv.org/pdf/2006.14799.pdf) and [slides](https://nlg-world.github.io/Part-IV-Evaluation.pdf)
* GATT, Albert; KRAHMER, Emiel. Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. Journal of Artificial Intelligence Research, v. 61, p. 65-170, 2018. [link](https://www.jair.org/index.php/jair/article/view/11173)
* HOWCROFT, David M. et al. Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions. In: Proceedings of the 13th International Conference on Natural Language Generation. 2020. p. 169-182. [link](https://www.aclweb.org/anthology/2020.inlg-1.23/)
* NOVIKOVA, Jekaterina et al. Why We Need New Evaluation Metrics for NLG. In: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. 2017. p. 2241-2252. [link](https://www.aclweb.org/anthology/D17-1238.pdf)
* REITER, Ehud; BELZ, Anja. An investigation into the validity of some metrics for automatically evaluating natural language generation systems. Computational Linguistics, v. 35, n. 4, p. 529-558, 2009. [link](https://www.mitpressjournals.org/doi/abs/10.1162/coli.2009.35.4.35405)
* VAN DER LEE, Chris et al. Best practices for the human evaluation of automatically generated text. In: Proceedings of the 12th International Conference on Natural Language Generation. 2019. p. 355-368. [link](https://www.aclweb.org/anthology/W19-8643.pdf)


Speech Synthesis

* LE MAGUER, Sébastian. Speech Synthesis Evaluation. Lecture in Saarland University, 2020. [link](http://www.coli.uni-saarland.de/courses/sprachsynthese/2020_SS/slides/SLM_tts_evaluation.pdf)
* WAGNER, Petra et al. Speech Synthesis Evaluation—State-of-the-Art Assessment and Suggestion for a Novel Research Program. In: Proceedings of the 10th Speech Synthesis Workshop (SSW10). 2019. [link](https://www.isca-speech.org/archive/SSW_2019/pdfs/SSW10_O_3-2.pdf)
