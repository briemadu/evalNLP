---
title: Schedule
layout: default
navigation_weight: 3
---

## Weekly Schedule

**Contact me if you are interested on the slides.**

*CORRETION: If you downloaded the slides a while ago, in slides 13 and 14 about statistical significance testing it should be >= (instead of >) in the if statement of the algorithms.*

|Week| Date | Focus    |  Slides  |  
|----|----------|----------|----------|
| 1 |04.11.2020 | Introduction / Course guidelines | |
| 2 |11.11.2020 | Paradigms |  |
| 3 |18.11.2020 | Common procedures |  |
| 4 |25.11.2020 | Annotation |  |
| 5 |02.12.2020 | Metrics & Measurements |   |
| 6 |09.12.2020 | Statistical significance testing |  |
| 7 |16.12.2020 | Best practices / Presentation guidelines |  |
| Break |  |  |   
| 8 |06.01.2021 | Presentation preparation |  
| 9 |13.01.2021 | Group 1: Machine Translation & History of Evaluation in NLP |  
|10 |20.01.2021 | Group 2: Natural Language Generation & Shared Tasks   |  
| 11 |27.01.2021 | Group 3: Dialogue & Replication Crisis |
| 12 |03.02.2021 | Group 4: Speech Synthesis & Ethics |  
| 13 |10.02.2021 | Tutorial / Project guidelines |  |


## Weekly Reading List

**Introduction**

1. COHEN, Paul R.; HOWE, Adele E. How evaluation guides AI research: The message still counts more than the medium. AI magazine, v. 9, n. 4, p. 35-35, 1988. [link](https://doi.org/10.1609/aimag.v9i4.952)
2. KING, Margaret. Evaluating natural language processing systems. Communications of the ACM, v. 39, n. 1, p. 73-79, 1996. [link](https://dl.acm.org/doi/abs/10.1145/234173.234208?casa_token=8KZTFYtxcXoAAAAA:x71qSj6riN8FnypzsDWZ3n8qIV8b0C5H14ToxWdQlLAYMqbWytVdAxmELv0QhyvuYfgCFFHhcYK_)

**Paradigms**

1. SPÄRCK JONES, Karen. Towards better NLP system evaluation. In: Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994. [link](https://www.aclweb.org/anthology/H94-1018.pdf)
2. HIRSCHMAN, Lynette; THOMPSON, Henry S. Overview of evaluation in speech and natural language processing. In: Survey of the state of the art in human language technology. Cambridge University Press, 1997. [link](http://www.dfki.de/~hansu/HLT-Survey.pdf) (pages 409-414)
3. BELZ, Anja. That's nice… what can you do with it?. Computational Linguistics, v. 35, n. 1, p. 111-118, 2009. [link](https://www.mitpressjournals.org/doi/pdf/10.1162/coli.2009.35.1.111)

**Common Procedures**

1. POTTS, Christopher. Evaluation methods in NLP. 2020. [link](https://nbviewer.jupyter.org/github/cgpotts/cs224u/blob/master/evaluation_methods.ipynb)
2. Communicating results with scientific graphs. The University of Queensland. [link](https://www.clips.edu.au/displaying-data/) (click on each type of plot to see presentation best practices)
3. REITER, Ehud. Use proper baselines. Blogpost, 2018. [link](https://ehudreiter.com/2018/08/30/use-proper-baselines/)
4. (Optional) Wikipedia's overview on cross validation. [link](https://en.wikipedia.org/wiki/Cross-validation_(statistics))

**Annotation**

1. SCHNEIDER, Nathan. What I’ve learned about annotating informal text (and why you shouldn’t take my word for it). In: Proceedings of The 9th Linguistic Annotation Workshop. 2015. p. 152-157. [link](https://www.aclweb.org/anthology/W15-1618.pdf)
2. BENDER, Emily M.; FRIEDMAN, Batya. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, v. 6, p. 587-604, 2018. [link](https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00041) (sections 1-5)
3. FORT, Karën; ADDA, Gilles; COHEN, K. Bretonnel. Amazon mechanical turk: Gold mine or coal mine?. Computational Linguistics, v. 37, n. 2, p. 413-420, 2011. [link](https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00057)
4. (Optional) FORT, Karën. Corpus Linguistics: Inter-Annotator Agreements (slides), 2011. [link](https://www.cs.brandeis.edu/~cs140b/CS140b_slides/CS140_Lect_8_fromFort_inist_CorpusLinguistics_05_iaa.pdf)

**Metrics and Measurements**
1. POTTS, Christopher. Evaluation metrics in NLP. 2020. [Read](https://nbviewer.jupyter.org/github/cgpotts/cs224u/blob/master/evaluation_metrics.ipynb) or [watch](https://www.youtube.com/watch?v=YygGzfkhtJc).
2. REITER, Ehud. Why do we still use 18-year old BLEU? / Small differences in BLEU are meaningless / Learning does not require evaluation metrics. Blogposts, 2018 and 2020. [link1](https://ehudreiter.com/2020/03/02/why-use-18-year-old-bleu/) [link2](https://ehudreiter.com/2020/07/28/small-differences-in-bleu-are-meaningless/) [link3](https://ehudreiter.com/2018/05/30/learning-does-not-require-metrics/)
3. HUYEN, Chip. Evaluation Metrics for Language Modeling. Blogpost, 2019. [link](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/)
4. (Optional) Slides on Information Theory metrics. [link](https://snlp2018.github.io/slides/information-theory-handout.pdf)
5. (Optional) Slides on Minimum Edit Distance algorithm. [link](https://web.stanford.edu/class/cs124/lec/med.pdf)
6. (Optional) Learning curves for diagnosing machine learning performance. [link](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/)

**Statistical Significance Testing**

1. Chapters 2, 3 and 4 of DROR, Rotem; PELED-COHEN, Lotem; SHLOMOV, Segev & REICHART, Roi. Statistical Significance Testing for Natural Language Processing. Synthesis Lectures on Human Language Technologies, v. 13, n. 2, p. 1-116, 2020. [link](https://www.morganclaypool.com/doi/abs/10.2200/S00994ED1V01Y202002HLT045?casa_token=sFwy5BwuTSYAAAAA:wfzPR418bHmo7Pt_T1LEzL2SSVI648i2MIFGFEwNK1NKglsjq8cOYMRfHmPCk8Qo0EsW4pi9vvij) (pages 3-33)
2. (Optional) DROR, R., BAUMER, G., SHLOMOV, S., & REICHART, R. The hitchhiker’s guide to testing statistical significance in natural language processing. In: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), p. 1383-1392, 2018. [link](https://www.aclweb.org/anthology/P18-1128/)
3. (Optional) SØGAARD, Anders; JOHANNSEN, Anders; PLANK, Barbara; HOVY, Dirk & MARTINEZ, Hector. What’s in a p-value in NLP?. In: Proceedings of the eighteenth conference on computational natural language learning, p. 1-10, 2014. [link](https://www.aclweb.org/anthology/W14-1601.pdf)
4. (Optional) BERG-KIRKPATRICK, Taylor; BURKETT, David; KLEIN, Dan. An empirical investigation of statistical significance in nlp. In: Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, p. 995-1005, 2012. [link](https://www.aclweb.org/anthology/D12-1091.pdf)
5. (Optional) William Morgan's slides about Approximate Randomization. [link](https://cs.stanford.edu/people/wmorgan/sigtest.pdf)
6. (Optional) KÖHN, Arne. We need to talk about significance tests. Blogpost, 2019. [link](https://arne.chark.eu/2019/we-need-to-talk-about-significance-tests/)

**Best Practices**

1. HOVY, Dirk; SPRUIT, Shannon L. The social impact of natural language processing. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2016. p. 591-598. [link](https://www.aclweb.org/anthology/P16-2096.pdf)
2. REITER; Ehud. My guidelines to evaluating AI systems, 2017, and Do people “cheat” by overfitting test data, 2020. Blogposts. [link1](https://ehudreiter.com/2017/11/21/guidelines-evaluating-ai-systems/) and [link2](https://ehudreiter.com/2020/02/06/cheat-by-overfitting-test-data/)
3. HEAVEN, Will Douglas. AI is wrestling with a replication crisis. MIT Technology Review, 2020. [link](https://www.technologyreview.com/2020/11/12/1011944/artificial-intelligence-replication-crisis-science-big-tech-google-deepmind-facebook-openai/)
4. GRUS, Joel. Reproducibility as a Vehicle for Engineering Best Practices. ICRL, 2019. [link](https://slideslive.com/38915876/reproducibility-as-a-vehicle-for-engineering-best-practices) (the first 19 minutes)
5. (Optional) LIPTON, Zachary C.; STEINHARDT, Jacob. Troubling trends in machine learning scholarship. Queue, v. 17, n. 1, p. 45-77, 2019. [link](https://queue.acm.org/detail.cfm?id=3328534)
6. (Optional) WIRED. Artificial intelligence confronts a reproducibility crisis, 2019. [link](https://www.wired.com/story/artificial-intelligence-confronts-reproducibility-crisis/)
7. (Optional) BENDER, Emily. The #BenderRule: On Naming the Languages We Study and Why It Matters. Blogpost, 2019. [link](https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/)
8. More material about Ethics in NLP. [link](https://aclweb.org/aclwiki/Ethics_in_NLP)
